---
layout: post  
title: "XJTU-ELEC427304 Big Data Notes"  
date: 2024-06-04 19:10 +0800  
last_modified_at: 2024-06-09 19:18 +0800  
tags: [Course Note]  
math: true  r
toc: true 
excerpt: "Course notes of XJTU-ELEC427304 (TBC)."
---

# Introduction

**PAC - probably Approximately Correct learning model** 

$$
P(|f(x)-y|\le \epsilon)\ge 1-\delta
$$

**Data Types** 
+ Continuous, Binary
+ Discrete, String
+ Symbolic

**Big Data** is high-volume, high-velocity, high-variety, demanding cost-effective, innovative forms of imformation processing, whose size is beyongd the ability of typical database software tools to capture, store, manage, and analyze.

**Data Mining** is the process of discovering patterns in large data set, including intersection of ML, statistic and database systems.

ä¹ é¢˜1ï¼šå¤§æ•°æ®çš„ç‰¹ç‚¹åŒ…æ‹¬ç±»å‹å¤šã€å¯¹å¤„ç†å®æ—¶æ€§è¦æ±‚é«˜ã€å®¹é‡å¤§

ä¹ é¢˜2ï¼šç†æƒ³çš„æ•°æ®æŒ–æ˜ç®—æ³•å¾—åˆ°çš„ç»“æœåº”è¯¥æ˜¯ï¼šUseful, Hidden, Interesting

**Source & Materials**
[KDnuggets](https://www.kdnuggets.com/), [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.php)

**Framework of ML**

+ Step 1: function with unknown 
$$
y=f_\theta(x)
$$
+ Step 2: define loss from training data 
$$
L(\theta)
$$
+ Step 3: optimization 
$$
\theta^* = \arg \min_{\theta} \mathcal{L}
$$

**DM Techniques - Classification**
Decision Trees, K-Nearest Neighbours, Neural Networks, Support Vector Machines

# Evaluation method of model

## Data Segmentation

**hold-out ç•™å‡ºæ³•** ä¿æŒæ•°æ®åˆ†å¸ƒä¸€è‡´æ€§ï¼Œå¤šæ¬¡é‡å¤éšæœºåˆ’åˆ†ï¼Œæµ‹è¯•é›†ä¸èƒ½å¤ªå¤§ã€ä¸èƒ½å¤ªå°

**cross-validation äº¤å‰éªŒè¯æ³•** kæŠ˜äº¤å‰éªŒè¯

**boostrapping è‡ªåŠ©æ³•** æœ‰æ”¾å›/å¯é‡å¤é‡‡æ ·ï¼Œæ•°æ®åˆ†å¸ƒæœ‰æ‰€æ”¹å˜ï¼Œè®­ç»ƒé›†ä¸åŸæ ·æœ¬é›†åŒè§„æ¨¡
ï¼ˆåŒ…å¤–ä¼°è®¡ out-of-bag estimationï¼‰

## Model performance
**Error rate é”™è¯¯ç‡** 
$$
E(ğ‘“;ğ·)=\frac{1}{m}\Sigma_{i=1}^{m}I(f(ğ’™_i)â‰ ğ‘¦)
$$

**accuracy ç²¾åº¦** 
$$
Acc(ğ‘“;ğ·)=\frac{1}{m}\Sigma_{i=1}^{m}I(f(ğ’™_i)=ğ‘¦)=1-E(ğ‘“;ğ·)
$$

**Precision æŸ¥å‡†ç‡** 
$$
P=\frac{TP}{TP+FP}
$$

**recall æŸ¥å…¨ç‡** 
$$
R=\frac{TP}{TP+FN}
$$

**F1** 
$$
\frac{1}{F_1}=\frac{1}{2}(\frac{1}{R}+\frac{1}{P})
$$

**FÎ²** 
$$
\frac{1}{F_\beta}=\frac{1}{1+\beta^2}(\frac{\beta^2}{R}+\frac{1}{P})
$$

**Confusion Matrix æ··æ·†çŸ©é˜µ**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/ConfusionMatrix.png?raw=true" width="100%">

**P-Ræ›²çº¿**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/pr.png?raw=true" width="100%">

**BEP å¹³è¡¡ç‚¹ Break-Even Point** æŸ¥å‡†ç‡ = æŸ¥å…¨ç‡çš„å–å€¼

**ROC å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/roc.png?raw=true" width="100%">

**AUC**ï¼šROCæ›²çº¿ä¸‹é¢ç§¯

How to Construct an ROC curve

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/rocCurve.png?raw=true" width="100%">

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/rocCurve2.png?raw=true" width="100%">

**Cost Matrix**
Cost-sensitive error rate and cost curve

**DM Techniques - Classification**:Â K-Means, Sequential Leader, Affinity Propagation

**DM Techniques â€“ Association Rule**

**DM Techniques â€“ Regression**

**Overfitting**Â 

# Data Preprocessing

## Conditional IndependenceÂ Â Â 
**Independent** â‰  **Uncorrelated**

e.g. When \\( X \in [-1, 1] \\), then \\( Y = x^2 \\). 

\\( \text{Cov}(X, Y) = 0 \\) indicating that \\( X \\) and \\( Y \\) are **uncorrelated**, even though \\( Y \\) is **completely determined** by \\( X \\).


## ID3 Framework

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/ID3Framework.png?raw=true" width="100%">


+ å¶èŠ‚ç‚¹ä¸ºçº¯èŠ‚ç‚¹ï¼Œstop.

+ å¶èŠ‚ç‚¹ä¸ºç©ºèŠ‚ç‚¹ï¼Œå¦‚ä½•å†³å®šè¯¥èŠ‚ç‚¹çš„åˆ†ç±»ï¼Ÿæ ¹æ®çˆ¶èŠ‚ç‚¹çš„æ¯”ä¾‹

+ å±æ€§ç”¨å®Œäº†ä¹Ÿæ— æ³•åˆ†ç±»ï¼šæ ‘æ— æ³•ç”Ÿé•¿ï¼Œæ ¹æ®å ä¼˜æ¯”ä¾‹å†³å®šç±»åˆ«ã€‚

**å¥¥å¡å§†å‰ƒåˆ€**ï¼šå€¾å‘äºé€‰æ‹©æ›´ç®€å•çš„æ¨¡å‹

**over fitting**ï¼šæ³›åŒ–è¯¯å·®å°ï¼Œæµ‹è¯•è¯¯å·®å¤§ï¼ˆé¢å¤–åˆ’å‡ºValidation Setæ¥åˆ¤å®šæ˜¯å¦è¿‡æ‹Ÿåˆï¼‰

è§£å†³ï¼šæ—©åœï¼ŒLossåŠ å…¥æ­£åˆ™é¡¹ï¼Œåˆ©ç”¨æ–°æ¨¡å‹

å†³ç­–æ ‘è§£å†³è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼š**å‰ªæ**
+ é¢„å‰ªæï¼šç”Ÿæˆå†³ç­–æ ‘æ—¶ï¼Œå¯¹æ¯ä¸ªèŠ‚ç‚¹åœ¨åˆ’åˆ†å‰å…ˆè¿›è¡Œä¼°è®¡ï¼Œè‹¥ä¸èƒ½å¸¦æ¥æ³›åŒ–æ€§èƒ½çš„æå‡ï¼Œåˆ™åœæ­¢åˆ’åˆ†ã€‚ï¼ˆä¸»è¦ç”¨åˆ°çš„æ ‡å‡†æ˜¯éªŒè¯é›†çš„ç²¾åº¦ï¼‰
+ åå‰ªæ:ç”Ÿæˆå®Œæ•´çš„å†³ç­–æ ‘ï¼Œè‡ªåº•å‘ä¸Šå‘å¶èŠ‚ç‚¹è¿›è¡Œè€ƒå¯Ÿï¼Œè‹¥è¯¥èŠ‚ç‚¹å¯¹åº”çš„å­æ ‘æ›¿æ¢æˆå¶èŠ‚ç‚¹èƒ½å¸¦æ¥æ³›åŒ–æ€§èƒ½çš„æå‡ï¼Œåˆ™æ›¿æ¢æˆå¶èŠ‚ç‚¹ã€‚

**Entropy Bias**

ä¿¡æ¯å¢ç›Šç‡
\\( \text{Cov}(X, Y) = 0 \\) indicating that \\( X \\) and \\( Y \\) are **uncorrelated**, even though \\( Y \\) is **completely determined** by \\( X \\).

# Optimization

# Convolutional Neural Networks

## Backpropagation åå‘ä¼ æ’­

## FUlly connected layers
å‚æ•°é‡

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/bd_fc_num.png?raw=true" width="100%">

## Convolution arithmetic å·ç§¯
## Pooling arithmetic æ± åŒ–

# Classifier
## Linear Classifier
## SVM æ”¯æŒå‘é‡æœº
$ y_i(w\cdot x_i+b)-1\ge 0 $ <br>
$ a_i \ge 0 $ <br>
$ a_i[y_i(w\cdot x_i+b)-1]=0 $ <br>

åªæœ‰æ”¯æŒå‘é‡åœ¨èµ·ä½œç”¨

## Soft Margin