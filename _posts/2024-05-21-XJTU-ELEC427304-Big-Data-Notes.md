---
layout: post  
title: "XJTU-ELEC427304 Big Data Notes"  
date: 2024-06-04 19:10 +0800  
last_modified_at: 2024-06-09 19:18 +0800  
tags: [Course Note]  
math: true  r
toc: true 
excerpt: "Course notes of XJTU-ELEC427304 (TBC)."
---
<style>
        h1 { font: 26pt Times !important; }
        h2 { font: 20pt Times !important; }
        h3 { font: 16pt Times !important; }
</style>

# Introduction

**PAC - probably Approximately Correct learning model**

$$
P(|f(x)-y|\le \epsilon)\ge 1-\delta
$$

**Data Types**

+ Continuous, Binary
+ Discrete, String
+ Symbolic

**Big Data** is high-volume, high-velocity, high-variety, demanding cost-effective, innovative forms of imformation processing, whose size is beyongd the ability of typical database software tools to capture, store, manage, and analyze.

**Data Mining** is the process of discovering patterns in large data set, including intersection of ML, statistic and database systems.

ä¹ é¢˜1ï¼šå¤§æ•°æ®çš„ç‰¹ç‚¹åŒ…æ‹¬ç±»å‹å¤šã€å¯¹å¤„ç†å®æ—¶æ€§è¦æ±‚é«˜ã€å®¹é‡å¤§

ä¹ é¢˜2ï¼šç†æƒ³çš„æ•°æ®æŒ–æ˜ç®—æ³•å¾—åˆ°çš„ç»“æœåº”è¯¥æ˜¯ï¼šUseful, Hidden, Interesting

**Source & Materials**
[KDnuggets](https://www.kdnuggets.com/), [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.php)

**Framework of ML**

+ Step 1: function with unknown

$$
y=f_\theta(x)
$$

+ Step 2: define loss from training data

$$
L(\theta)
$$

+ Step 3: optimization

$$
\theta^* = \arg \min_{\theta} \mathcal{L}
$$

**DM Techniques - Classification**
Decision Trees, K-Nearest Neighbours, Neural Networks, Support Vector Machines

# Evaluation method of model

## Data Segmentation

**hold-out ç•™å‡ºæ³•** ä¿æŒæ•°æ®åˆ†å¸ƒä¸€è‡´æ€§ï¼Œå¤šæ¬¡é‡å¤éšæœºåˆ’åˆ†ï¼Œæµ‹è¯•é›†ä¸èƒ½å¤ªå¤§ã€ä¸èƒ½å¤ªå°

**cross-validation äº¤å‰éªŒè¯æ³•** kæŠ˜äº¤å‰éªŒè¯

**boostrapping è‡ªåŠ©æ³•** æœ‰æ”¾å›/å¯é‡å¤é‡‡æ ·ï¼Œæ•°æ®åˆ†å¸ƒæœ‰æ‰€æ”¹å˜ï¼Œè®­ç»ƒé›†ä¸åŸæ ·æœ¬é›†åŒè§„æ¨¡
ï¼ˆåŒ…å¤–ä¼°è®¡ out-of-bag estimationï¼‰

## Model performance

**Error rate é”™è¯¯ç‡**

$$
E(ğ‘“;ğ·)=\frac{1}{m}\sum_{i=1}^{m}I(f(ğ’™_i)â‰ ğ‘¦)
$$

**Accuracy ç²¾åº¦**

$$
Acc(ğ‘“;ğ·)=\frac{1}{m}\sum_{i=1}^{m}I(f(ğ’™_i)=ğ‘¦)=1-E(ğ‘“;ğ·)
$$

**Precision æŸ¥å‡†ç‡**

$$
P=\frac{TP}{TP+FP}
$$

**Recall æŸ¥å…¨ç‡**

$$
R=\frac{TP}{TP+FN}
$$

**F1**

$$
\frac{1}{F_1}=\frac{1}{2}(\frac{1}{R}+\frac{1}{P})
$$

**FÎ²**

$$
\frac{1}{F_\beta}=\frac{1}{1+\beta^2}(\frac{\beta^2}{R}+\frac{1}{P})
$$

**Confusion Matrix æ··æ·†çŸ©é˜µ**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/ConfusionMatrix.png?raw=true" width="100%">

**P-Ræ›²çº¿**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/pr.png?raw=true" width="60%">

**BEP å¹³è¡¡ç‚¹ Break-Even Point** æŸ¥å‡†ç‡ = æŸ¥å…¨ç‡çš„å–å€¼

**ROC å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/roc.png?raw=true" width="100%">

**AUC**ï¼šROCæ›²çº¿ä¸‹é¢ç§¯

How to Construct an ROC curve

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/rocCurve.png?raw=true" width="100%">

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/rocCurve2.png?raw=true" width="100%">

**Cost Matrix**
Cost-sensitive error rate and cost curve

C(i,j)ï¼šCost of misclassifying class j example as class i

**Cost VS Accuracy**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/cost_vs_accuracy.png?raw=true" width="100%">

**Lift Analysis**

[[æ¨¡å‹è§£é‡‹ç­–ç•¥]Lift Chart, Permutation Importance, LIME](https://yulongtsai.medium.com/lift-chart-permutation-importance-lime-c22be8bdaf48)

ä¾‹é¢˜ï¼šå‡è®¾ç›®æ ‡å®¢æˆ·å äººç¾¤çš„5%ï¼Œç°æ ¹æ®ç”¨æˆ·æ¨¡å‹è¿›è¡Œæ‰“åˆ†æ’åºï¼Œ
å–1000åæ½œåœ¨å®¢æˆ·ä¸­æ’åå‰10%çš„å®¢æˆ·ï¼Œå‘ç°å…¶ä¸­åŒ…å«25åç›®
æ ‡å®¢æˆ·ï¼Œé—®æ­¤æ¨¡å‹åœ¨10%å¤„çš„æå‡åº¦æ˜¯å¤šå°‘?

ç­”æ¡ˆï¼š5

ä¾‹é¢˜ï¼šæˆ‘ä»¬é€šå¸¸å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†è¿›è¡Œæ¨¡å‹çš„è®­
ç»ƒï¼Œå‚æ•°çš„éªŒè¯éœ€è¦åœ¨**éªŒè¯é›†**ä¸Šè¿›è¡Œï¼Œå‚æ•°ç¡®å®šå**éœ€è¦**é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

ä¾‹é¢˜ï¼šå½“è¥¿ç“œæ”¶è´­å…¬å¸å»ç“œæ‘Šæ”¶è´­è¥¿ç“œæ—¶æ—¢å¸Œæœ›æŠŠå¥½ç“œéƒ½æ”¶èµ°åˆä¿è¯æ”¶
åˆ°çš„ç“œä¸­åç“œå°½å¯èƒ½çš„å°‘ï¼Œè¯·é—®ä»–åº”è¯¥è€ƒè™‘ä»€ä¹ˆè¯„ä»·æŒ‡æ ‡ï¼Ÿ

æ­£ç¡®ï¼š**F1è°ƒå’Œå¹³å‡**ä¸**BEP**

ä¾‹é¢˜ï¼šå‡è®¾æˆ‘ä»¬å·²ç»å»ºç«‹å¥½äº†ä¸€ä¸ªäºŒåˆ†ç±»æ¨¡å‹,è¾“å‡ºæ˜¯0æˆ–1,åˆå§‹é˜ˆå€¼è®¾
ç½®ä¸º05 è¶…è¿‡0.5æ¦‚ç‡ä¼°è®¡å°±åˆ¤åˆ«ä¸º1,å¦åˆ™å°±åˆ¤åˆ«ä¸º0;å¦‚æœæˆ‘ä»¬ç°
åœ¨ç”¨å¦ä¸€ä¸ªå¤§äº0.5çš„é˜ˆå€¼ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œä¸‹åˆ—è¯´æ³•æ­£ç¡®çš„æ˜¯ï¼š**æŸ¥å‡†ç‡ä¼šä¸Šå‡æˆ–ä¸å˜ï¼ŒæŸ¥å…¨ç‡ä¼šä¸‹é™æˆ–ä¸å˜**

**DM Techniques - Classification**:Â K-Means, Sequential Leader, Affinity Propagation

**DM Techniques â€“ Association Rule**

**DM Techniques â€“ Regression**

# Data Preprocessing
**Typical lssues** : ç¼ºå°‘å±æ€§å€¼Missing Attribute Values, ä¸åŒçš„ç¼–ç /å‘½åæ–¹æ¡ˆDifferent Coding/Naming Schemes, ä¸å¯è¡Œçš„å€¼Infeasible Values, ä¸ä¸€è‡´çš„æ•°æ®InconsistentData, å¼‚å¸¸å€¼Outliers

**Data Quality** : Accuracy ï¼ˆå‡†ç¡®æ€§ï¼‰, Completenessï¼ˆå®Œæ•´æ€§ï¼‰, Consistency ï¼ˆä¸€è‡´æ€§ï¼‰, Interpretabilityï¼ˆå¯è§£é‡Šæ€§ï¼‰, Credibilityï¼ˆå¯ä¿¡æ€§ï¼‰, Timelinessï¼ˆæ—¶æ•ˆæ€§ï¼‰

**æ•°æ®é›†æˆ** : ç»„åˆæ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®ã€‚

**æ•°æ®ç¼©å‡** : ç‰¹å¾é€‰æ‹©ã€æŠ½æ ·

**Privacy**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/privacy.png?raw=true" width="100%">

**No Free Lunch**

Why bother so many different algorithms?

+ No algorithm is always superior to others.
+ No parameter settingis optimal over all problems.

Look for the best match between problem and algorithm.

+ Experience
+ Trial and Error

Factors to consider:
+ Applicability
+ Computational Complexity
+ Interpretability

Always start with simple ones.

OUTLINES of Data Preprocessing
## Data Cleaning æ•°æ®æ¸…æ´—
å¡«å……ç¼ºå¤±å€¼ã€æ›´æ­£ä¸ä¸€è‡´çš„æ•°æ®ã€è¯†åˆ«å¼‚å¸¸å€¼å’Œå™ªå£°æ•°æ®ã€‚
### ç¼ºå¤±å€¼

æ•°æ®ç¼ºå¤±ç±»å‹åˆ†ä¸ºä¸‰ç§ï¼šå®Œå…¨éšæœºç¼ºå¤±ã€éšæœºç¼ºå¤±ã€ééšæœºç¼ºå¤±ã€‚
å‚è€ƒ[æ•°æ®ç¼ºå¤±ç±»å‹](https://blog.csdn.net/jwtning/article/details/116125819)

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/missing.png?raw=true" width="100%">

å¦‚ä½•å¤„ç†ç¼ºå¤±æ•°æ®ï¼Ÿ
+ å¿½ç•¥ï¼šåˆ é™¤æœ‰ç¼ºå¤±å€¼çš„æ ·æœ¬/å±æ€§ï¼Œæœ€ç®€å•ã€æœ€ç›´æ¥çš„æ–¹æ³•ï¼Œä½ç¼ºå¤±ç‡æ—¶æ•ˆæœå¾ˆå¥½
+ æ‰‹åŠ¨å¡«å†™ç¼ºå¤±å€¼ï¼šé‡æ–°æ”¶é›†æ•°æ®æˆ–é¢†åŸŸçŸ¥è¯†ï¼Œç¹ç/ä¸å¯è¡Œ
+ è‡ªåŠ¨å¡«å†™ç¼ºå¤±å€¼ï¼šå…¨å±€å¸¸æ•°/å¹³å‡å€¼æˆ–ä¸­ä½æ•°/æœ€å¯èƒ½çš„å€¼

ä¾‹é¢˜ï¼šå­¦ç”Ÿå°æ˜åœ¨è°ƒæŸ¥é—®å·ä¸­æ²¡æœ‰å›ç­”ä¸‹è¿°é—®é¢˜:â€œä½ å»å¹´çš„å·¥èµ„æ”¶å…¥å’Œå‰å¹´ç›¸æ¯”æ˜¯å¦æœ‰æ‰€å¢åŠ ?â€å¯¹è¿™ç§æƒ…å†µæœ€æ°å½“çš„æè¿°æ˜¯:N/A

ä»¥ä¸‹å‚è€ƒ[åŠ‰æ™ºçš“ (Chih-Hao Liu) æ©Ÿå™¨å­¸ç¿’_å­¸ç¿’ç­†è¨˜ç³»åˆ—(96)ï¼šå€åŸŸæ€§ç•°å¸¸å› å­(Local Outlier Factor)](https://tomohiroliu22.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98%E7%B3%BB%E5%88%97-96-%E5%8D%80%E5%9F%9F%E6%80%A7%E7%95%B0%E5%B8%B8%E5%9B%A0%E5%AD%90-local-outlier-factor-a141c2450d4a)

**Outliersç¦»ç¾¤ç‚¹** : Outliersâ‰ Anomaly

**Local Outliner Factor**
å…³äºLOFç®—æ³•ï¼Œå®ƒæ˜¯åŸºäºç©ºé—´å¯†åº¦æ¥å¯»æ‰¾å¼‚å¸¸å€¼çš„ï¼Œè¿™é‡Œæˆ‘ä»¬å®šä¹‰**å¯è¾¾è·ç¦»reachability distance** = max(Bç‚¹åˆ°ç¦»Bç¬¬kè¿‘çš„ç‚¹çš„è·ç¦», Aå’ŒBçš„è·ç¦»)

$$
reachability_{-}distance_{k}(A,B)=m a x\Big[k_{-}distance(B),distance(A,B)\Big]
$$ 
å‡è®¾æœ‰ä¸¤ä¸ªç‚¹Aå’ŒBï¼Œ`k_distance(B)`ä»£è¡¨çš„å°±æ˜¯Bç‚¹åˆ°ç¦»Bç¬¬kè¿‘çš„ç‚¹çš„è·ç¦»ï¼Œ`distance(A,B)`åˆ™å°±æ˜¯Aå’ŒBçš„è·ç¦»ã€‚æ‰€ä»¥è¿™é‡Œçš„æ„æ€æ˜¯ï¼šå¦‚æœç‚¹å’Œç‚¹ä¹‹é—´ç›¸è·å¤Ÿè¿‘ï¼Œå°±å°†ä»–ä»¬ä¸€è§†åŒä»ï¼Œè§†ä¸ºå¯†åº¦è¾ƒé«˜çš„åŒºåŸŸã€‚

è€Œæ¥ä¸‹ä¾†æˆ‘å€‘æœƒè¨ˆç®—**local reachability density: (å¹³å‡è·ç¦»)**

$$
IRD_{k}(A)=\frac{1}{\left( \frac{\sum_{B\in{\cal N}_{k}(A)}reachability_{-}distance_{k}(A,B)}{|{N}_{k}(A)|}\right)}
$$

å…¶ä¸­N_kç‚ºAé»çš„neighborã€‚æ‰€ä»¥é€™å€‹å¼å­ä»£è¡¨çš„å°±æ˜¯ï¼Œæˆ‘å€‘Aé»neighborå…¶reachability distance**å¹³å‡**çš„**å€’æ•¸**ï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥èªªï¼Œ**å¦‚æœIRDå¾ˆå¤§ï¼Œä»£è¡¨ä»¥Aé»ç‚ºä¸­å¿ƒçš„å€åŸŸå¾ˆå¯†é›†**ï¼Œåä¹‹å‰‡æ˜¯å¾ˆç–é¬†ã€‚
è€Œç•¶æˆ‘å€‘æ±‚å¾—äº†IRDä¹‹å¾Œï¼Œæˆ‘å€‘æœ€å¾Œå°±æœƒè¨ˆç®—

**Local Outlier Factor**:

$$L O F_{k}(A)=\frac{\sum_{B\in N_{k}(A)}I R D_{k}(B)/I R D_{k}(A)}{\left|N_{k}(A)\right|}=\frac{1}{I R D_{k}(A)}\frac{\sum_{B\in N_{k}(A)}I R D_{k}(B)}{\left|N_{k}(A)\right|}$$

æˆ‘å€‘å¯ä»¥çœ‹åˆ°LOFï¼Œä»–åšçš„äº‹æƒ…å°±æ˜¯è¨ˆç®—Aæ‰€æœ‰**neighborçš„**IRDå€¼ä¸¦ä¸”å°‡å…¶å¹³å‡é™¤ä»¥IRD(A)ã€‚è€ŒLOFåœ¨æ„ç¾©ä¸Šä¾†èªªï¼Œ**å¦‚æœæ¥è¿‘1ä»£è¡¨ï¼ŒAå’Œå…¶Neighborçš„ç©ºé–“å¯†åº¦éƒ½éå¸¸æ¥è¿‘ï¼Œå¦‚æœå°æ–¼1éå¸¸å¤šï¼Œä»£è¡¨Açš„å¯†åº¦å¤§æ–¼ä»–çš„neighborï¼Œä¹Ÿå°±æ˜¯å¯†åº¦è¼ƒé«˜çš„å€åŸŸï¼Œè‹¥å¤§æ–¼1éå¸¸å¤šï¼Œå‰‡ä»£è¡¨Açš„å¯†åº¦å°æ–¼ä»–çš„neighborã€‚**

**ç›¸ä¼¼åº¦ä¸ä¸ç›¸ä¼¼åº¦** 
+ ç›¸ä¼¼åº¦: ä¸¤ä¸ªæ•°æ®å¯¹è±¡ç›¸ä¼¼ç¨‹åº¦çš„æ•°å€¼æµ‹é‡,å¯¹è±¡è¶Šç›¸ä¼¼ï¼Œç›¸ä¼¼åº¦è¶Šé«˜,é€šå¸¸åœ¨ [0,1] èŒƒå›´å†…
+ ç›¸å¼‚åº¦: ä¸¤ä¸ªæ•°æ®å¯¹è±¡å·®å¼‚ç¨‹åº¦çš„æ•°å€¼æµ‹é‡,å¯¹è±¡è¶Šç›¸ä¼¼ï¼Œç›¸ä¼¼åº¦è¶Šä½,æœ€å°ä¸ç›¸ä¼¼åº¦é€šå¸¸ä¸º 0,ä¸Šé™å„ä¸ç›¸åŒ

ç®€å•å±æ€§çš„ç›¸ä¼¼æ€§/ä¸ç›¸ä¼¼æ€§

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/simidissimi.png?raw=true" width="100%">

**distance**
	
+ Euclidean Distance
$$dist={\sqrt{\frac{n}{k-1}}}(p_{k}-q_{k})^{2}$$
+ Minkowski Distance
$$dist=(\sum_{k=1}^{n}|p_{k}-q_{k}|^r)^{1/r}$$

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/Minkowski.png?raw=true" width="100%">

+ Mahalanobis Distance

$$mahalanobis(p,q)=\sqrt{(p-q)\Sigma^{-1}\left(p-q\right)^{r}}$$

$$
\Sigma_{j,k}=\frac{1}{n-1} \sum_{i=1}^{n}(X_{ij}-\overline{{X}}_{j})(X_{ik}-\overline{{X}}_{j})
$$

Duplicate Dataé‡å¤æ•°æ®å¤„ç†æ–¹æ³•

Large Data : create keys -> sort -> merge

## Data Transformation æ•°æ®è½¬æ¢
ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ— é”™è¯¯çš„æ•°æ®é›†ï¼Œè¿˜éœ€è¦æ ‡å‡†åŒ– standardizedã€‚ç±»å‹è½¬æ¢ã€è§„èŒƒåŒ–Normalizationã€é‡‡æ ·ã€ èšåˆAggregation

ä¾‹å¦‚ï¼šé€‰å–å˜æ¢$$\phi(x)$$å°†éçº¿æ€§å˜ä¸ºçº¿æ€§ã€ä½¿ç”¨ç‹¬çƒ­ç¼–ç æ ‡è®°ç±»åˆ«(ä¸èƒ½å› ä¸ºç¼–ç è€Œäº§ç”Ÿæ–°çš„å‚æ•°å½±å“ï¼Œå¦‚å› ä¸ºç¼–ç 1 2 3è€Œå¯¼è‡´è·ç¦»ä¸ç›¸ç­‰)ç­‰

**é‡‡æ ·**ï¼šä»€ä¹ˆæ˜¯é‡‡æ ·ï¼Ÿ

æœ‰æ•ˆæŠ½æ ·çš„å…³é”®åŸåˆ™å¦‚ä¸‹ï¼š
+ å¦‚æœæ ·æœ¬å…·æœ‰ä»£è¡¨æ€§ï¼Œåˆ™ä½¿ç”¨æ ·æœ¬çš„æ•ˆæœå‡ ä¹ä¸ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†ä¸€æ ·å¥½ 
+ å¦‚æœæ ·æœ¬å…·æœ‰ä¸åŸå§‹æ•°æ®é›†å¤§è‡´ç›¸åŒçš„å±æ€§ï¼ˆæ„Ÿå…´è¶£çš„ï¼‰ï¼Œåˆ™è¯¥æ ·æœ¬å…·æœ‰ä»£è¡¨æ€§

**Imbalanced Datasetsä¸å¹³è¡¡çš„æ•°æ®é›†**

**G-means**

$${G-m e a n}=(A c c^{+}\times A c c^{-})^{1/2}$$

$$w h e r e\ A c c^{+}={\frac{T P}{T P+F N}};\ \ \ A c c^{-}={\frac{T N}{T N+F P}}$$

**F-measure**

$$F-m e a s u r e={\frac{2\times P r e c i s i o n\times R e c a l l}{P r e c i s i o n+R e c a l l}}$$

$$w h e r e\;\;\;P r e c i s i o n=\frac{T P}{T P+F P};\;\;\;\;{R e c a l l}=\frac{T P}{T P+F N}=A c c^{+}$$

**Over-Sampling**

å¯ä»¥å‚è€ƒ[æœºå™¨å­¦ä¹ ä¹‹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ (3) â€”â€” é‡‡æ ·æ–¹æ³•
](https://www.cnblogs.com/massquantity/p/9382710.html),è¿™é‡Œé¢çš„Border-line SMOTEä¸ªäººè®¤ä¸ºæ¯”è¾ƒç¬¦åˆpptä¸Šçš„ä¸‹ä¸€ä¸ªè¦ç‚¹**Boundary Sampling**

ä¸å¹³è¡¡çš„æ•°æ®é›†è¦é‡‡æ ·ã€æ‰©å¢ã€è°ƒæ•´loss function

**Normalization**

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/Normalization.png?raw=true" width="100%">


## Data Description

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/description.png?raw=true" width="80%">
e
<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/description2.png?raw=true" width="100%">

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/description3.png?raw=true" width="100%">e

## Feature Selection

## Feature Extraction


## Conditional IndependenceÂ Â Â 


**Independent** â‰  **Uncorrelated**

e.g. When \\( X \in [-1, 1] \\), then \\( Y = x^2 \\).

\\( \text{Cov}(X, Y) = 0 \\) indicating that \\( X \\) and \\( Y \\) are **uncorrelated**, even though \\( Y \\) is **completely determined** by \\( X \\).

## ID3 Framework

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/ID3Framework.png?raw=true" width="100%">

+ å¶èŠ‚ç‚¹ä¸ºçº¯èŠ‚ç‚¹ï¼Œstop.
+ å¶èŠ‚ç‚¹ä¸ºç©ºèŠ‚ç‚¹ï¼Œå¦‚ä½•å†³å®šè¯¥èŠ‚ç‚¹çš„åˆ†ç±»ï¼Ÿæ ¹æ®çˆ¶èŠ‚ç‚¹çš„æ¯”ä¾‹
+ å±æ€§ç”¨å®Œäº†ä¹Ÿæ— æ³•åˆ†ç±»ï¼šæ ‘æ— æ³•ç”Ÿé•¿ï¼Œæ ¹æ®å ä¼˜æ¯”ä¾‹å†³å®šç±»åˆ«ã€‚

**å¥¥å¡å§†å‰ƒåˆ€**ï¼šå€¾å‘äºé€‰æ‹©æ›´ç®€å•çš„æ¨¡å‹

**over fitting**ï¼šæ³›åŒ–è¯¯å·®å°ï¼Œæµ‹è¯•è¯¯å·®å¤§ï¼ˆé¢å¤–åˆ’å‡ºValidation Setæ¥åˆ¤å®šæ˜¯å¦è¿‡æ‹Ÿåˆï¼‰

è§£å†³ï¼šæ—©åœï¼ŒLossåŠ å…¥æ­£åˆ™é¡¹ï¼Œåˆ©ç”¨æ–°æ¨¡å‹

å†³ç­–æ ‘è§£å†³è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼š**å‰ªæ**

+ é¢„å‰ªæï¼šç”Ÿæˆå†³ç­–æ ‘æ—¶ï¼Œå¯¹æ¯ä¸ªèŠ‚ç‚¹åœ¨åˆ’åˆ†å‰å…ˆè¿›è¡Œä¼°è®¡ï¼Œè‹¥ä¸èƒ½å¸¦æ¥æ³›åŒ–æ€§èƒ½çš„æå‡ï¼Œåˆ™åœæ­¢åˆ’åˆ†ã€‚ï¼ˆä¸»è¦ç”¨åˆ°çš„æ ‡å‡†æ˜¯éªŒè¯é›†çš„ç²¾åº¦ï¼‰
+ åå‰ªæ:ç”Ÿæˆå®Œæ•´çš„å†³ç­–æ ‘ï¼Œè‡ªåº•å‘ä¸Šå‘å¶èŠ‚ç‚¹è¿›è¡Œè€ƒå¯Ÿï¼Œè‹¥è¯¥èŠ‚ç‚¹å¯¹åº”çš„å­æ ‘æ›¿æ¢æˆå¶èŠ‚ç‚¹èƒ½å¸¦æ¥æ³›åŒ–æ€§èƒ½çš„æå‡ï¼Œåˆ™æ›¿æ¢æˆå¶èŠ‚ç‚¹ã€‚

**Entropy Bias**

ä¿¡æ¯å¢ç›Šç‡
\\( \text{Cov}(X, Y) = 0 \\) indicating that \\( X \\) and \\( Y \\) are **uncorrelated**, even though \\( Y \\) is **completely determined** by \\( X \\).

# Optimization

# Convolutional Neural Networks

## Backpropagation åå‘ä¼ æ’­

## FUlly connected layers

å‚æ•°é‡

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/bd_fc_num.png?raw=true" width="100%">

## Convolution arithmetic å·ç§¯

## Pooling arithmetic æ± åŒ–

# Classifier

## Linear Classifier

## SVM æ”¯æŒå‘é‡æœº

$$ y_i(w\cdot x_i+b)-1\ge 0 $$

$$ a_i \ge 0 $$ 

$$ a_i[y_i(w\cdot x_i+b)-1]=0 $$

åªæœ‰æ”¯æŒå‘é‡åœ¨èµ·ä½œç”¨

### Soft Margin

$$
y_{i}(w x_{i}+\partial)-1+\xi_{i}\geq0
$$

$$\Phi(w)=\frac{1}{2}\,w^{t}w+C\sum_{i}\xi_{i}$$

$$\xi_{r}\geq0$$

$${\cal L}_{P} \equiv\frac{1}{2}\Bigl|w\Bigr|^{2}+C\sum_{i=1}^{l}\xi_{i}-\sum_{i=1}^{l}\alpha_{i}[y_{i}(w\cdot x_{i}+b)-1+\xi_{i}]-\sum_{i=1}^{l}\mu_{i}\xi_{i}$$

### éçº¿æ€§SVMs ï¼ˆNon-linear SVMsï¼‰

**Feature Space**ï¼š
å‘é«˜é˜¶ç©ºé—´è¿›è¡Œæ˜ å°„ï¼Œå¯ä»¥æŠŠå¾ˆå¤šä¸èƒ½ç”¨linear SVMè§£å†³çš„é—®é¢˜ä½¿ç”¨linear SVMè§£å†³

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/nonlinearSVMs.png?raw=true" width="100%">

Which kind of $$\varphi(x)$$ can solve this problem?

**Kernel Trick** 

$$K(x_{i}x_{j})=\varphi(x_{i})*\varphi(x_{j})$$

$$\mathcal{\Phi}\colon\,x\longrightarrow\mathcal{\varphi}(x)$$

$$x_{i}*x_{j}\longrightarrow\varphi(x_{i})*\varphi(x_{j})$$

**Solutions of w & b**


é—®é¢˜ï¼šåœ¨SVMå½“ä¸­è¿›è¡Œç©ºé—´æ˜ å°„çš„ä¸»è¦ç›®çš„æ˜¯:
- A é™ä½è®¡ç®—å¤æ‚åº¦ ï¼ˆæé«˜ï¼‰
- B æå–è¾ƒä¸ºé‡è¦çš„ç‰¹å¾
- C å¯¹åŸå§‹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
- D æé«˜åŸå§‹é—®é¢˜çš„å¯åˆ†æ€§ âˆš

é—®é¢˜ï¼šå¯¹äºSVMï¼Œåœ¨æ˜ å°„åçš„é«˜ç»´ç©ºé—´ç›´æ¥è¿›è¡Œè®¡ç®—çš„ä¸»è¦é—®é¢˜æ˜¯:
- A æ¨¡å‹å¯è§£é‡Šæ€§å·®
- B è®¡ç®—å¤æ‚åº¦é«˜ âˆš
- C å®¹æ˜“å‡ºç°å¥‡å¼‚çŸ©é˜µ
- D å®¹æ˜“å‡ºç°ç¨€ç–çŸ©é˜µ

é—®é¢˜ï¼šæ‰€è°“kernel trickï¼ŒæŒ‡çš„æ˜¯:
- A åˆ©ç”¨åœ¨åŸå§‹ç©ºé—´å®šä¹‰çš„å‡½æ•°æ›¿ä»£é«˜ç»´ç©ºé—´çš„å‘é‡å†…ç§¯æ“ä½œ âˆš
- B åˆ©ç”¨åœ¨é«˜ç»´ç©ºé—´å®šä¹‰çš„å‡½æ•°æ›¿ä»£åŸå§‹ç©ºé—´çš„å‘é‡å†…ç§¯æ“ä½œ
- C æ ¸å‡½æ•°çš„å¯¼æ•°å…·æœ‰ç®€å•çš„è§£æè§£ï¼Œç®€åŒ–äº†è¿ç®—
- D æ ¸å‡½æ•°å…·æœ‰å›ºå®šçš„ä¸Šä¸‹ç•Œï¼Œå¯ä»¥è¾“å‡º$$(-1,+1)$$åŒºé—´ä¸­çš„è¿ç»­å€¼



# é›†æˆå­¦ä¹  

## bagging

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/bootstrap.png?raw=true" width="100%">

## æœ‰æ”¾å›é‡‡æ ·
æœ‰å¤šå°‘æ ·æœ¬æ²¡æœ‰è¢«å‹‡æ•¢ä½†æ˜¯è¢«ä»¥ä¸ºæ˜¯å¯ä»¥testçš„ OOB
å¥½å¤„ï¼šå¯ä»¥å¸®æˆ‘ä»¬æ„å»ºå…·æœ‰åˆ†æ•£æ€§çš„åŸºç¡€åˆ†ç±»å™¨
å……åˆ†åˆ©ç”¨æ‰€æœ‰æ ·æœ¬

## ä¿è¯åŸºç¡€åˆ†ç±»å™¨å¤šæ ·æ€§çš„æ–¹æ³•
+ ç®—æ³•å¤šæ ·æ€§
+ è®­ç»ƒé›†ï¼ˆéšæœºåˆæ”¾å›é‡‡æ ·ï¼‰
+ é€‰æ‹©ä¸åŒçš„å±æ€§ï¼Œå†³ç­–æ ‘ä¸­ç”¨$$\sqrt{K}$$ä¸ªå±æ€§æ„é€ 500-5000æ£µæ ‘
+ è¶…å‚æ•°


## Stack

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/stack.png?raw=true" width="100%">

<img src="https://github.com/Sihan0229/Sihan0229.github.io/blob/master/assets/stacking.png?raw=true" width="100%">

## Boosting
 **Adaboost**

